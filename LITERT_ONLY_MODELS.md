# üî• AION - Modelos LiteRT-LM √önicos

## ‚úÖ Status: Sistema Simplificado com LiteRT-LM

O projeto AION agora usa **EXCLUSIVAMENTE** modelos **Google LiteRT-LM** com infer√™ncia multimodal real (Vis√£o + Texto) e GPU acceleration.

---

## üéØ Decis√£o de Design

### Por que apenas LiteRT-LM?

1. **‚úÖ Infer√™ncia Neural Real** - N√£o √© simula√ß√£o, √© IA de verdade
2. **‚úÖ GPU Acceleration Nativa** - Performance m√°xima no dispositivo
3. **‚úÖ Modelos Verificados** - Todos dispon√≠veis no Hugging Face
4. **‚úÖ Simplicidade** - Um √∫nico sistema, sem complexidade h√≠brida
5. **‚úÖ Suporte Oficial Google** - Mantido pela equipe Android AI
6. **‚úÖ Menor Tamanho** - Modelos ultra compactos (584MB - 3.39GB)

### Remo√ß√£o do llama.cpp

O sistema anterior suportava modelos llama.cpp, mas foi simplificado pelos seguintes motivos:
- ‚ùå Maior complexidade (c√≥digo C++ nativo)
- ‚ùå Modelos maiores (0.9GB - 4.4GB)
- ‚ùå Performance inferior em GPU
- ‚ùå Multimodalidade requer integra√ß√£o manual
- ‚ùå Manuten√ß√£o comunit√°ria (n√£o oficial)

---

## üì¶ Modelos Dispon√≠veis

### 1. Gemma 3 1B (LiteRT) üî• **RECOMENDADO**

| Propriedade | Valor |
|------------|-------|
| **ID** | gemma3-1b-it |
| **Nome** | Gemma 3 1B (LiteRT) üî• |
| **Tamanho** | 584 MB |
| **Par√¢metros** | 1B |
| **Quantiza√ß√£o** | int4 |
| **Reposit√≥rio** | litert-community/Gemma3-1B-IT |
| **Arquivo** | gemma3-1b-it-int4.litertlm |

**Caracter√≠sticas:**
- ‚úÖ Ultra leve e r√°pido
- ‚úÖ Perfeito para a maioria dos dispositivos
- ‚úÖ Baixo consumo de mem√≥ria
- ‚úÖ Excelente balan√ßo velocidade/precis√£o
- ‚úÖ Recomendado como padr√£o

**Download:**
```
https://huggingface.co/litert-community/Gemma3-1B-IT/resolve/main/gemma3-1b-it-int4.litertlm
```

---

### 2. Gemma 3n E2B (LiteRT) ‚ö°

| Propriedade | Valor |
|------------|-------|
| **ID** | gemma-3n-e2b-it |
| **Nome** | Gemma 3n E2B (LiteRT) ‚ö° |
| **Tamanho** | 3.39 GB |
| **Par√¢metros** | 2B (eficiente 5B) |
| **Quantiza√ß√£o** | int4 |
| **Reposit√≥rio** | google/gemma-3n-E2B-it-litert-lm |
| **Arquivo** | gemma-3n-E2B-it-int4.litertlm |

**Caracter√≠sticas:**
- ‚úÖ Alta precis√£o com 2B par√¢metros eficientes
- ‚úÖ Arquitetura Matformer avan√ßada
- ‚úÖ Melhor compreens√£o de contexto
- ‚úÖ Para dispositivos com 4GB+ RAM
- ‚úÖ Suporte multimodal completo (texto, imagem, √°udio)

**Download:**
```
https://huggingface.co/google/gemma-3n-E2B-it-litert-lm/resolve/main/gemma-3n-E2B-it-int4.litertlm
```

---

## üèóÔ∏è Arquitetura Simplificada

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   AION System                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  User Task   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  AIControlService    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Screenshot  ‚îÇ     ‚îÇ                      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ  ‚îÇ LiteRT         ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ  ‚îÇ Multimodal     ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ  ‚îÇ Controller     ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ           ‚îÇ          ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ           ‚ñº          ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ  ‚îÇ LiteRT Engine  ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ  ‚îÇ (GPU Accel)    ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ           ‚îÇ          ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ           ‚ñº          ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ     ‚îÇ AIAction ‚îÇ     ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ   ‚îÇ
‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Fluxo Simplificado:**
1. Usu√°rio define tarefa + screenshot √© capturado
2. AIControlService usa LiteRTMultimodalController
3. Modelo LiteRT-LM faz infer√™ncia com GPU
4. Retorna AIAction parseada
5. A√ß√£o √© executada via Accessibility Service

---

## üíª C√≥digo Simplificado

### Estrutura de Dados

```kotlin
data class LocalVisionModel(
    val id: String,
    val name: String,
    val description: String,
    val repoId: String,
    val modelFilename: String,
    val mmProjFilename: String,  // Vazio para LiteRT
    val estimatedSize: String,
    val isLiteRT: Boolean = true  // Sempre true agora
)

val AVAILABLE_LOCAL_MODELS = listOf(
    LocalVisionModel(
        id = "gemma3-1b-it",
        name = "Gemma 3 1B (LiteRT) üî•",
        description = "ULTRA LEVE! Modelo multimodal oficial do Google...",
        repoId = "litert-community/Gemma3-1B-IT",
        modelFilename = "gemma3-1b-it-int4.litertlm",
        mmProjFilename = "",
        estimatedSize = "584 MB",
        isLiteRT = true
    ),
    LocalVisionModel(
        id = "gemma-3n-e2b-it",
        name = "Gemma 3n E2B (LiteRT) ‚ö°",
        description = "Modelo multimodal avan√ßado com 2B par√¢metros...",
        repoId = "google/gemma-3n-E2B-it-litert-lm",
        modelFilename = "gemma-3n-E2B-it-int4.litertlm",
        mmProjFilename = "",
        estimatedSize = "3.39 GB",
        isLiteRT = true
    )
)
```

### AIControlService (Simplificado)

```kotlin
class AIControlService : Service() {
    private var liteRTController: LiteRTMultimodalController? = null
    
    fun setupLocalAI(modelPath: String, mmProjPath: String, isLiteRT: Boolean = true) {
        serviceScope.launch {
            liteRTController = LiteRTMultimodalController(modelPath)
            val loaded = liteRTController?.initialize() ?: false
            
            if (loaded) {
                addLog("‚úÖ Modelo LiteRT-LM carregado com sucesso (GPU acceleration)")
            } else {
                addLog("‚ùå Erro ao carregar modelo LiteRT-LM")
                liteRTController = null
            }
        }
    }
    
    fun isModelLoaded(): Boolean {
        return liteRTController?.isReady() ?: false
    }
    
    // Execu√ß√£o de tarefa usa apenas liteRTController
    val action = liteRTController?.analyzeScreenAndDecide(
        screenshot,
        task,
        conversationHistory
    )
}
```

### LocalModelManager (Simplificado)

```kotlin
fun isModelDownloaded(model: LocalVisionModel): Boolean {
    val modelFile = getModelFile(model)
    
    // LiteRT models s√£o unit√°rios (sem mmproj)
    if (model.isLiteRT) {
        return downloader.checkModelExists(modelFile)
    }
    
    // C√≥digo legacy (n√£o usado atualmente)
    val mmProjFile = getMMProjFile(model)
    return downloader.checkModelExists(modelFile) && 
           downloader.checkModelExists(mmProjFile)
}
```

---

## üöÄ Como Usar

### Para o Usu√°rio Final

1. Abra o app AION
2. V√° na aba **"Modelos"**
3. Escolha **Gemma 3 1B (LiteRT) üî•** (recomendado)
4. Toque em **"Baixar Modelo"**
5. Aguarde o download (584 MB)
6. Toque em **"Selecionar"**
7. Volte para **"Principal"**
8. Digite uma tarefa e execute!

### Para Desenvolvedores

```kotlin
// Carregar modelo
val controller = LiteRTMultimodalController(
    "/data/data/com.aion.aicontroller/files/vision_models/gemma3-1b-it-int4.litertlm"
)
val success = controller.initialize()

// Fazer infer√™ncia
if (success) {
    val action = controller.analyzeScreenAndDecide(
        screenshot = screenshotBitmap,
        task = "Abrir o Chrome",
        conversationHistory = listOf()
    )
    
    // Executar a√ß√£o
    when (action?.type) {
        ActionType.CLICK -> performClick(action.x, action.y)
        ActionType.TYPE_TEXT -> typeText(action.text)
        ActionType.SCROLL -> scroll(action.direction)
        // ...
    }
}

// Limpar
controller.unload()
```

---

## üìä Compara√ß√£o de Performance

| Modelo | Tamanho | Velocidade | Precis√£o | Mem√≥ria | GPU | Recomendado |
|--------|---------|-----------|----------|---------|-----|-------------|
| **Gemma 3 1B** | 584 MB | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 1.5 GB | ‚úÖ | ‚úÖ Maioria |
| **Gemma 3n E2B** | 3.39 GB | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 3.5 GB | ‚úÖ | ‚úÖ Alta precis√£o |

---

## üîß Configura√ß√£o T√©cnica

### Depend√™ncia Gradle

```gradle
implementation("com.google.ai.edge.litert:litert-llm:1.2.0")
```

### Requisitos do Sistema

- **Android:** 12+ (API 31+)
- **RAM:** 2GB m√≠nimo, 4GB recomendado
- **Armazenamento:** 600MB - 4GB (dependendo do modelo)
- **GPU:** Qualquer GPU moderna (acelera√ß√£o autom√°tica)
- **Processador:** ARM64 ou x86_64

### Formatos de Modelo

- **Extens√£o:** `.litertlm`
- **Quantiza√ß√£o:** int4 (4-bit integer)
- **Arquitetura:** Gemma 3 / Gemma 3n
- **Multimodal:** ‚úÖ Texto + Imagem + √Åudio

---

## üìÅ Arquivos do Projeto

### Arquivos Principais

1. **Models.kt** - Define os 2 modelos LiteRT dispon√≠veis
2. **LiteRTMultimodalController.kt** - Controller de infer√™ncia
3. **AIControlService.kt** - Servi√ßo simplificado (apenas LiteRT)
4. **MainActivity.kt** - UI simplificada sem badges LiteRT
5. **PreferencesManager.kt** - Modelo padr√£o: gemma3-1b-it
6. **LocalModelManager.kt** - Gerencia download (unit√°rio para LiteRT)
7. **HuggingFaceDownloader.kt** - Download do Hugging Face

### Arquivos Legacy (N√£o Usados)

- `LocalAIController.kt` - Antigo controller llama.cpp
- `LocalVisionInference.kt` - Infer√™ncia llama.cpp (C++)
- `VisionAIController.kt` - Antigo controller de vis√£o

---

## üéâ Benef√≠cios da Simplifica√ß√£o

### Antes (Sistema H√≠brido)
- ‚ö†Ô∏è 10 modelos diferentes (llama.cpp + LiteRT)
- ‚ö†Ô∏è L√≥gica complexa if/else (isUsingLiteRT)
- ‚ö†Ô∏è M√∫ltiplos controllers (3 controllers diferentes)
- ‚ö†Ô∏è Download em 2 partes (model + mmproj)
- ‚ö†Ô∏è Modelos grandes (at√© 4.4GB)
- ‚ö†Ô∏è Performance vari√°vel

### Agora (Sistema Simplificado)
- ‚úÖ 2 modelos verificados (LiteRT-LM apenas)
- ‚úÖ C√≥digo limpo e direto
- ‚úÖ Um √∫nico controller (LiteRTMultimodalController)
- ‚úÖ Download unit√°rio (apenas .litertlm)
- ‚úÖ Modelos compactos (584MB - 3.39GB)
- ‚úÖ GPU acceleration garantida

---

## üîó Refer√™ncias

- **LiteRT-LM GitHub:** https://github.com/google-ai-edge/LiteRT-LM
- **Google AI Edge:** https://ai.google.dev/edge/litert
- **Gemma 3 1B:** https://huggingface.co/litert-community/Gemma3-1B-IT
- **Gemma 3n E2B:** https://huggingface.co/google/gemma-3n-E2B-it-litert-lm
- **LiteRT Documentation:** https://github.com/google-ai-edge/litert

---

## üìù Notas de Migra√ß√£o

### Usu√°rios Existentes

Se voc√™ tinha modelos llama.cpp instalados:
1. Eles ainda existem no dispositivo em `/files/vision_models/`
2. Podem ser deletados manualmente se necess√°rio
3. O app n√£o os usar√° mais automaticamente
4. Baixe um modelo LiteRT para continuar usando o AION

### Desenvolvedores

Se voc√™ estava usando o c√≥digo antigo:
1. `LocalAIController` foi substitu√≠do por `LiteRTMultimodalController`
2. `setupLocalAI()` agora sempre usa LiteRT (par√¢metro `isLiteRT` sempre true)
3. `isModelLoaded()` verifica apenas `liteRTController`
4. Modelos n√£o precisam mais de `mmProjPath` (campo vazio)

---

**Implementado por:** Capy AI Agent
**Data:** 2025-10-05
**Vers√£o AION:** 2.1 - LiteRT-LM Only
